{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit #to calculate logit \n",
    "import scipy.io as si\n",
    "import numpy as np\n",
    "\n",
    "#.mat file has two matrix X (5000*400) and y(5000*1)\n",
    "File = si.loadmat('ex4data1.mat')\n",
    "\n",
    "#sigmoid function implementation\n",
    "def sigmoid_function(z):\n",
    "          return expit(z)\n",
    "\n",
    "#sigmoid gradient implementation\n",
    "def sigmoid_gradient(Z):\n",
    "    temp = sigmoid_function(Z)\n",
    "    return (temp*(1 - temp))\n",
    "\n",
    "#implement Forward Propagation\n",
    "def compute_input_layer(theta1,input_layer):\n",
    "    return (sigmoid_function(np.dot(input_layer,theta1.T)))\n",
    "\n",
    "def compute_hidden_layer(theta2,A):\n",
    "    return (sigmoid_function(np.dot(A,theta2.T)))\n",
    "\n",
    "#return 5000*10 matrix\n",
    "def compute_hypothesis(Theta1,Theta2,input_layer):\n",
    "    \n",
    "    #get 5000*25 matrix\n",
    "    A1 = compute_input_layer(Theta1,input_layer)\n",
    "    \n",
    "    #change A dim to 5000*26\n",
    "    A1 = np.insert(A1,0,1,axis = 1)\n",
    "\n",
    "    #get 5000*10 matrix\n",
    "    A2 = compute_hidden_layer(Theta2,A1)\n",
    "    return A2 , A1\n",
    "\n",
    "#implement Cost function for Neural Nertwork with no Regularization\n",
    "\n",
    "#return the actual result of Kth classifier for a given data in training set\n",
    "def get_result(actual_result,K,data):\n",
    "    if actual_result[data] == K:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#return the value for Kth classifier for a given data in training set\n",
    "def get_hypo_value_K(hypothesis,data,K):\n",
    "        return (hypothesis[data].item(K))\n",
    "\n",
    "#implement Cost function for Neural Nertwork with  Regularization\n",
    "def Cost_function_reg(t_theta1,t_theta2,input_layer,actual_result,K,Lambda):\n",
    "    \n",
    "    #create theta1 and theta2\n",
    "    theta1 = t_theta1\n",
    "    theta1 = np.delete(theta1,0,1)\n",
    "    theta2 = t_theta2\n",
    "    theta2 = np.delete(theta2,0,1)\n",
    "    \n",
    "    #calculate hypothesis\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(t_theta1,t_theta2,input_layer)\n",
    "    \n",
    "    cost = 0\n",
    "    #iterate it for each training data\n",
    "    for i in range(len(hypothesis1)):\n",
    "        #for each class\n",
    "        for j in range(K):\n",
    "            term1 =  - get_result(actual_result,j,i)*np.log(get_hypo_value_K(hypothesis1,i,j)) \n",
    "            term2 =  - (1 - get_result(actual_result,j,i))*np.log(1 - get_hypo_value_K(hypothesis1,i,j)) \n",
    "            cost = cost + term1 + term2\n",
    "            \n",
    "    cost = (cost/(len(hypothesis1)))\n",
    "    \n",
    "    #calculate Regularizarion Term\n",
    "    \n",
    "    #minimize each theta presents in all_theta dictionary\n",
    "\n",
    "    #regularize theta1\n",
    "    t1 = np.sum(theta1*theta1) \n",
    "    \n",
    "    #regularize theta2\n",
    "    t2 = np.sum(theta2*theta2)\n",
    "    \n",
    "    Reg_term = (Lambda/(2*(len(hypothesis1)))) * (t1 + t2)\n",
    "   \n",
    "    #update cost\n",
    "    cost = cost + Reg_term\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input_layer\n",
    "input_layer = np.array(File['X'])\n",
    "input_layer = np.insert(input_layer,0,1,axis = 1)\n",
    "\n",
    "#create actual result of training set in form of {0,1}\n",
    "def result_binary(actual_result,num_labels):\n",
    "    y = np.zeros((len(actual_result),num_labels))\n",
    "    for i in range(len(actual_result)):\n",
    "        temp = actual_result[i]\n",
    "        y[i][temp - 1] = 1\n",
    "    return y\n",
    "\n",
    "output_layer = result_binary(File['y'],10)\n",
    "\n",
    "#randomly initializes theata1(25,401) and theta2(10,26) for a given epsilon\n",
    "\n",
    "\n",
    "epsilon = 0.12\n",
    "init_theta1 = np.random.uniform(-epsilon,epsilon,(25,401))\n",
    "init_theta2 = np.random.uniform(-epsilon,epsilon,(10,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement BackPropagation\n",
    "def BackPropagation_Reg(init_theta1,init_theta2,input_layer,output_layer,Lambda):\n",
    "    \n",
    "    t1 = init_theta1\n",
    "    t2 = init_theta2\n",
    "    \n",
    "    #calculate forward propagation\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "    \n",
    "    #computes small delta\n",
    "    \n",
    "    #compute delta_3\n",
    "    delta_3 = hypothesis1 - output_layer\n",
    "    \n",
    "    #compute delta_2\n",
    "    term1 = np.dot(delta_3,init_theta2).T\n",
    "    term1 = np.delete(term1,0,0)\n",
    "    delta_2 = np.multiply(term1.T,sigmoid_gradient(np.dot(input_layer,init_theta1.T)))\n",
    "    \n",
    "    #compute gradient for layer 1 with regularization\n",
    "    grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "    grad_1 = grad_1/len(hypothesis1)\n",
    "    \n",
    "    t1 = np.delete(t1,0,1)\n",
    "    t1 = np.insert(t1,0,0,1)\n",
    "    \n",
    "    grad_1 = grad_1 + (Lambda/len(hypothesis1))*t1\n",
    "    \n",
    "    #compute gradient for layer 2\n",
    "    grad_2 = np.dot(hypothesis2.T,delta_3)\n",
    "    grad_2 = np.delete(grad_2,0,0).T\n",
    "    grad_2 = grad_2/len(hypothesis2)\n",
    "    \n",
    "    t2 = np.delete(t2,0,1)\n",
    "    grad_2 = grad_2 + (Lambda/len(hypothesis1))*t2\n",
    "    \n",
    "    \n",
    "    return (grad_1 , grad_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.2016731   0.64745088]\n",
      " [ 1.          0.23133295  0.08664872]\n",
      " [ 1.          0.56538822  0.15139938]\n",
      " [ 1.          0.38616217  0.97634764]\n",
      " [ 1.          0.40109214  0.23699668]]\n",
      "[[0]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "(4, 3)\n",
      "(3, 5)\n",
      "[ 0.0258074   0.11338325 -0.04833079 -0.119952   -0.11695457 -0.00280994\n",
      " -0.03501438  0.03343907  0.09152103  0.09980646 -0.00840855  0.01665259\n",
      "  0.05156353  0.00033068 -0.05419041  0.10510945  0.11694273  0.06280476\n",
      "  0.03257324  0.07114657  0.07815911 -0.02362363  0.03707092  0.04472612\n",
      "  0.02385372  0.03645262 -0.03674317]\n"
     ]
    }
   ],
   "source": [
    "#perform  Gradient Checking on a small neural network\n",
    "\n",
    "#make a small neural network\n",
    "\n",
    "#define sizes\n",
    "input_layer_size = 3 #including bias unit\n",
    "hidden_layer_size = 5 #including bias unit\n",
    "num_labels = 3\n",
    "m = 5\n",
    "\n",
    "#put some random values\n",
    "#randomly  initializes test_theta's with a range of (-.12,.12)\n",
    "epsilon = 0.12\n",
    "test_theta1 = np.random.uniform(-epsilon,epsilon,(hidden_layer_size -1,input_layer_size ))\n",
    "test_theta2 = np.random.uniform(-epsilon,epsilon,(num_labels,hidden_layer_size ))\n",
    "\n",
    "#randomly generate input layer (5*2)\n",
    "test_input_layer = np.random.rand(m,input_layer_size - 1)\n",
    "test_input_layer = np.insert(test_input_layer,0,1,axis = 1)\n",
    "print(test_input_layer)\n",
    "\n",
    "#randomly generate  actual result \n",
    "test_result = np.random.randint(0,3,(m,1))\n",
    "print(test_result)\n",
    "\n",
    "#create test actualresult in binary form\n",
    "test_actual_result = np.zeros((len(test_result),num_labels))\n",
    "for i in range(len(test_result)):\n",
    "         test_actual_result[i][test_result[i]] = 1\n",
    "\n",
    "print(test_actual_result)\n",
    "\n",
    "#make a long test_THETA VECTOR \n",
    "test_theta_L = np.concatenate((test_theta1.flatten(),test_theta2.flatten()))\n",
    "print(test_theta1.shape)\n",
    "print(test_theta2.shape)\n",
    "print(test_theta_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(test_theta_L,epsilon,num_labels,Lambda):\n",
    "    GRADIENT_CHECK = np.zeros((len(test_theta_L),1))\n",
    "    for i in range(len(test_theta_L)):\n",
    "        inc = np.zeros((len(test_theta_L),1))\n",
    "        inc[i] = inc[i] + epsilon\n",
    "    \n",
    "        theta_plus = test_theta_L + inc\n",
    "        theta_minus = test_theta_L - inc\n",
    "    \n",
    "        #unrolling for theta_plus\n",
    "        pass_theta1 = np.array(theta_plus[0:12]).reshape(4,3)\n",
    "        pass_theta2 = np.array(theta_plus[12:]).reshape(3,5)\n",
    "    \n",
    "        cost_plus = Cost_function_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels,Lambda)\n",
    "    \n",
    "        #unrolling for theta_minus\n",
    "        pass_theta1 = np.array(theta_minus[0:12]).reshape(4,3)\n",
    "        pass_theta2 = np.array(theta_minus[12:]).reshape(3,5)\n",
    "    \n",
    "        cost_minus = Cost_function_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels,Lambda)\n",
    "    \n",
    "        GRADIENT_CHECK[i] = (cost_plus - cost_minus)/(2*epsilon)\n",
    "    \n",
    "    return GRADIENT_CHECK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from Backpropagtion for theta 1 \n",
      "\n",
      "[[ 0.00465677  0.06911329 -0.02704999]\n",
      " [ 0.00245254 -0.07055993 -0.00194372]\n",
      " [ 0.00907769  0.02411864  0.05830718]\n",
      " [ 0.00024239 -0.00285066  0.01065836]]\n",
      "\n",
      "output from Gradient Checking for theta 1 \n",
      "\n",
      "[[ 0.00465677  0.06911329 -0.02704999]\n",
      " [ 0.00245254 -0.07055993 -0.00194372]\n",
      " [ 0.00907769  0.02411864  0.05830718]\n",
      " [ 0.00024239 -0.00285066  0.01065836]]\n",
      "\n",
      "output from Backpropagtion for theta 2 \n",
      "\n",
      "[[ 0.07072487  0.02797388  0.13217175  0.14127553]\n",
      " [ 0.08894143  0.10505101  0.11294098  0.05639876]\n",
      " [ 0.18739889  0.16132361  0.18270869  0.14523484]]\n",
      "\n",
      "output from Gradient Checking for theta 2 \n",
      "\n",
      "[[ 0.07072487  0.02797388  0.13217175  0.14127553]\n",
      " [ 0.08894143  0.10505101  0.11294098  0.05639876]\n",
      " [ 0.18739889  0.16132361  0.18270869  0.14523484]]\n"
     ]
    }
   ],
   "source": [
    "Lambda = 3\n",
    "#make calls to backpropagation and Cost_function for small neural network\n",
    "TEST_GRADIENT_1 , TEST_GRADIENT_2 = BackPropagation_Reg(test_theta1,test_theta2,test_input_layer,test_actual_result,Lambda)\n",
    "\n",
    "test_theta_L = test_theta_L.reshape(len(test_theta1.flatten()) + len(test_theta2.flatten()) ,1)\n",
    "GRADIENT_CHECK = gradient_checking(test_theta_L,.0001,num_labels,Lambda)\n",
    "#COMPARE OUTPUT OF GRADIENT_CHECKING AND BACKPROPAGATION\n",
    "\n",
    "GRADIENT_CHECK1 = np.array(GRADIENT_CHECK[0:12]).reshape(4,3)\n",
    "GRADIENT_CHECK2 = np.array(GRADIENT_CHECK[12:]).reshape(3,5)\n",
    "GRADIENT_CHECK2 = np.delete(GRADIENT_CHECK2,0,1)\n",
    "\n",
    "print(\"output from Backpropagtion for theta 1 \\n\")\n",
    "print(TEST_GRADIENT_1)\n",
    "print(\"\\noutput from Gradient Checking for theta 1 \\n\")\n",
    "print(GRADIENT_CHECK1)\n",
    "\n",
    "print(\"\\noutput from Backpropagtion for theta 2 \\n\")\n",
    "print(TEST_GRADIENT_2)\n",
    "print(\"\\noutput from Gradient Checking for theta 2 \\n\")\n",
    "print(GRADIENT_CHECK2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
