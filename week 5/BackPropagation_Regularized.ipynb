{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit #to calculate logit \n",
    "import scipy.io as si\n",
    "import numpy as np\n",
    "\n",
    "#.mat file has two matrix X (5000*400) and y(5000*1)\n",
    "File = si.loadmat('ex4data1.mat')\n",
    "\n",
    "#sigmoid function implementation\n",
    "def sigmoid_function(z):\n",
    "          return expit(z)\n",
    "\n",
    "#sigmoid gradient implementation\n",
    "def sigmoid_gradient(Z):\n",
    "    return (Z*(1 - Z))\n",
    "\n",
    "#implement Forward Propagation\n",
    "def compute_input_layer(theta1,input_layer):\n",
    "    return (sigmoid_function(np.dot(input_layer,theta1.T)))\n",
    "\n",
    "def compute_hidden_layer(theta2,A):\n",
    "    return (sigmoid_function(np.dot(A,theta2.T)))\n",
    "\n",
    "#return 5000*10 matrix\n",
    "def compute_hypothesis(Theta1,Theta2,input_layer):\n",
    "    \n",
    "    #get 5000*25 matrix\n",
    "    A1 = compute_input_layer(Theta1,input_layer)\n",
    "    \n",
    "    #change A dim to 5000*26\n",
    "    A1 = np.insert(A1,0,1,axis = 1)\n",
    "\n",
    "    #get 5000*10 matrix\n",
    "    A2 = compute_hidden_layer(Theta2,A1)\n",
    "    return A2 , A1\n",
    "\n",
    "#implement Cost function for Neural Nertwork with no Regularization\n",
    "\n",
    "#implement Cost function for Neural Nertwork with  Regularization\n",
    "def Cost_function_reg(t_theta1,t_theta2,input_layer,actual_result,K,Lambda):\n",
    "    \n",
    "    #create theta1 and theta2\n",
    "    theta1 = t_theta1\n",
    "    theta1 = np.delete(theta1,0,1)\n",
    "    theta2 = t_theta2\n",
    "    theta2 = np.delete(theta2,0,1)\n",
    "    \n",
    "    #calculate hypothesis\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(t_theta1,t_theta2,input_layer)\n",
    "    \n",
    "    \n",
    "    term1 =  - np.sum(np.multiply(actual_result,np.log(hypothesis1))) \n",
    "    term2 =  - np.sum(np.multiply((1 -actual_result),np.log(1 - hypothesis1))) \n",
    "    cost = term1 + term2\n",
    "    cost = (cost/(len(hypothesis1)))\n",
    "    \n",
    "    #calculate Regularizarion Term\n",
    "    \n",
    "    #minimize each theta presents in all_theta dictionary\n",
    "\n",
    "    #regularize theta1\n",
    "    t1 = np.sum(theta1*theta1) \n",
    "    \n",
    "    #regularize theta2\n",
    "    t2 = np.sum(theta2*theta2)\n",
    "    \n",
    "    Reg_term = (Lambda/(2*(len(hypothesis1)))) * (t1 + t2)\n",
    "   \n",
    "    #update cost\n",
    "    cost = cost + Reg_term\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input_layer\n",
    "input_layer = np.array(File['X'])\n",
    "input_layer = np.insert(input_layer,0,1,axis = 1)\n",
    "\n",
    "#create actual result of training set in form of {0,1}\n",
    "def result_binary(actual_result,num_labels):\n",
    "    y = np.zeros((len(actual_result),num_labels))\n",
    "    for i in range(len(actual_result)):\n",
    "        temp = actual_result[i]\n",
    "        y[i][temp - 1] = 1\n",
    "    return y\n",
    "\n",
    "output_layer = result_binary(File['y'],10)\n",
    "\n",
    "#randomly initializes theata1(25,401) and theta2(10,26) for a given epsilon\n",
    "\n",
    "\n",
    "epsilon = 0.12\n",
    "init_theta1 = np.random.uniform(-epsilon,epsilon,(25,401))\n",
    "init_theta2 = np.random.uniform(-epsilon,epsilon,(10,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement BackPropagation\n",
    "def BackPropagation_Reg(init_theta1,init_theta2,input_layer,output_layer,Lambda,m):\n",
    "\n",
    "    t1 = init_theta1\n",
    "    t2 = init_theta2\n",
    "   \n",
    "    #calculate forward propagation\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "    \n",
    "    #computes small delta\n",
    "    \n",
    "    #compute delta_3\n",
    "    delta_3 = hypothesis1 - output_layer\n",
    "    \n",
    "    #compute delta_2\n",
    "    term1 = delta_3.dot(init_theta2)\n",
    "    delta_2 = term1*(hypothesis2*(1 - hypothesis2))\n",
    "    delta_2 = np.delete(delta_2,0,1)\n",
    "   \n",
    "    #compute gradient for layer 1 with regularization\n",
    "    grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "    grad_1 = grad_1/m\n",
    "    \n",
    "    t1 = np.delete(t1,0,1)\n",
    "    t1 = np.insert(t1,0,0,1)\n",
    "    \n",
    "    grad_1 = grad_1 + (Lambda/m)*t1\n",
    "    \n",
    "    #compute gradient for layer 2\n",
    "    grad_2 = ((delta_3).transpose()).dot(hypothesis2)\n",
    "    grad_2 = grad_2/m\n",
    "    \n",
    "    t2 = np.delete(t2,0,1)\n",
    "    t2 = np.insert(t2,0,0,1)\n",
    "    \n",
    "    grad_2 = grad_2 + (Lambda/m)*t2\n",
    "    \n",
    "    \n",
    "    return (grad_1 , grad_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.0841471   0.09092974  0.014112  ]\n",
      " [ 1.         -0.07568025 -0.09589243 -0.02794155]\n",
      " [ 1.          0.06569866  0.09893582  0.04121185]\n",
      " [ 1.         -0.05440211 -0.09999902 -0.05365729]\n",
      " [ 1.          0.0420167   0.09906074  0.06502878]]\n",
      "[2 3 1 2 3]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "(5, 4)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "#perform  Gradient Checking on a small neural network\n",
    "\n",
    "#make a small neural network\n",
    "\n",
    "#define sizes\n",
    "input_layer_size = 3 #including bias unit\n",
    "hidden_layer_size = 5 #including bias unit\n",
    "num_labels = 3\n",
    "m = 5\n",
    "def debugInitializeWeights(fan_out,fan_in):\n",
    "    W = np.zeros((fan_out,fan_in + 1))\n",
    "    W = (np.sin(np.arange(1,np.size(W) + 1)).reshape(W.shape))/10\n",
    "    return W\n",
    "#put some random values\n",
    "#randomly  initializes test_theta's with a range of (-.12,.12)\n",
    "epsilon = 0.12\n",
    "test_theta1 = debugInitializeWeights(hidden_layer_size ,input_layer_size )\n",
    "test_theta2 = debugInitializeWeights(num_labels,hidden_layer_size )\n",
    "\n",
    "#randomly generate input layer (5*2)\n",
    "test_input_layer = debugInitializeWeights(m,input_layer_size - 1)\n",
    "test_input_layer = np.insert(test_input_layer,0,1,axis = 1)\n",
    "print(test_input_layer)\n",
    "\n",
    "#randomly generate  actual result \n",
    "test_result = 1 + np.remainder(np.arange(1,m+1),num_labels)\n",
    "print(test_result)\n",
    "\n",
    "#create test actualresult in binary form\n",
    "test_actual_result = np.zeros((m,num_labels))\n",
    "for i in range(m):\n",
    "         test_actual_result[i,test_result[i] - 1] = 1\n",
    "\n",
    "print(test_actual_result)\n",
    "\n",
    "#make a long test_THETA VECTOR \n",
    "\n",
    "print(test_theta1.shape)\n",
    "print(test_theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(test_theta1,test_theta2,epsilon,num_labels,Lambda,test_input_layer,test_result):\n",
    "    \n",
    "    GRADIENT_CHECK = np.zeros((len(test_theta1.flatten()) + len(test_theta2.flatten()),1))\n",
    "    test_theta_L = np.concatenate((test_theta1.flatten(),test_theta2.flatten()),0)\n",
    "    test_theta_L = test_theta_L.reshape(len(test_theta1.flatten()) + len(test_theta2.flatten()),1)\n",
    "    \n",
    "    for i in range(len(test_theta_L)):\n",
    "        inc = np.zeros((len(test_theta_L),1))\n",
    "        inc[i] = inc[i] + epsilon\n",
    "    \n",
    "        theta_plus = test_theta_L + inc\n",
    "        theta_minus = test_theta_L - inc\n",
    "    \n",
    "        #unrolling for theta_plus\n",
    "        pass_theta1 = np.array(theta_plus[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "        pass_theta2 = np.array(theta_plus[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "    \n",
    "        cost_plus = Cost_function_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels,Lambda)\n",
    "    \n",
    "        #unrolling for theta_minus\n",
    "        pass_theta1 = np.array(theta_minus[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "        pass_theta2 = np.array(theta_minus[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "    \n",
    "        cost_minus = Cost_function_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels,Lambda)\n",
    "    \n",
    "        GRADIENT_CHECK[i] = (cost_plus - cost_minus)/(2*epsilon)\n",
    "        \n",
    "    return GRADIENT_CHECK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from Backpropagtion for theta 1 \n",
      "\n",
      "[[  1.23162247e-02   1.18755982e+01   1.84328876e+00  -9.88373189e+00]\n",
      " [  3.92471369e-03  -3.64897631e+00   8.58046725e+00   1.29210688e+01]\n",
      " [ -8.08459407e-03  -7.10488439e+00  -1.30598939e+01  -7.00769717e+00]\n",
      " [ -1.26669105e-02   1.29371759e+01   8.49251369e+00  -3.76012648e+00]\n",
      " [ -5.59342547e-03  -9.80809348e+00   1.95715273e+00   1.19230017e+01]]\n",
      "\n",
      "output from Gradient Checking for theta 1 \n",
      "\n",
      "[[  1.23162246e-02   1.18755982e+01   1.84328876e+00  -9.88373189e+00]\n",
      " [  3.92471369e-03  -3.64897631e+00   8.58046725e+00   1.29210688e+01]\n",
      " [ -8.08459407e-03  -7.10488439e+00  -1.30598939e+01  -7.00769717e+00]\n",
      " [ -1.26669105e-02   1.29371759e+01   8.49251369e+00  -3.76012648e+00]\n",
      " [ -5.59342547e-03  -9.80809348e+00   1.95715273e+00   1.19230017e+01]]\n",
      "\n",
      "output from Backpropagtion for theta 2 \n",
      "\n",
      "[[  0.30934772  12.03649153   1.99006383  -9.72557201 -12.36593432\n",
      "   -3.50193005]\n",
      " [  0.108133    12.97718207   5.43421847  -7.05018037 -13.00456382\n",
      "   -6.95586705]\n",
      " [  0.10627037  12.99309317   8.543316    -3.7061368  -12.50111052\n",
      "   -9.75760049]]\n",
      "\n",
      "output from Gradient Checking for theta 2 \n",
      "\n",
      "[[  0.30934772  12.03649153   1.99006383  -9.72557201 -12.36593432\n",
      "   -3.50193005]\n",
      " [  0.108133    12.97718207   5.43421847  -7.05018037 -13.00456382\n",
      "   -6.95586705]\n",
      " [  0.10627037  12.99309317   8.543316    -3.7061368  -12.50111052\n",
      "   -9.75760049]]\n"
     ]
    }
   ],
   "source": [
    "Lambda = 653\n",
    "#make calls to backpropagation and Cost_function for small neural network\n",
    "TEST_GRADIENT_1 , TEST_GRADIENT_2 = BackPropagation_Reg(test_theta1,test_theta2,test_input_layer,test_actual_result,Lambda,m)\n",
    "\n",
    "GRADIENT_CHECK = gradient_checking(test_theta1,test_theta2,.0001,num_labels,Lambda,test_input_layer,test_actual_result)\n",
    "#COMPARE OUTPUT OF GRADIENT_CHECKING AND BACKPROPAGATION\n",
    "\n",
    "GRADIENT_CHECK1 = np.array(GRADIENT_CHECK[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "GRADIENT_CHECK2 = np.array(GRADIENT_CHECK[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "\n",
    "print(\"output from Backpropagtion for theta 1 \\n\")\n",
    "print(TEST_GRADIENT_1)\n",
    "print(\"\\noutput from Gradient Checking for theta 1 \\n\")\n",
    "print(GRADIENT_CHECK1)\n",
    "\n",
    "print(\"\\noutput from Backpropagtion for theta 2 \\n\")\n",
    "print(TEST_GRADIENT_2)\n",
    "print(\"\\noutput from Gradient Checking for theta 2 \\n\")\n",
    "print(GRADIENT_CHECK2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
