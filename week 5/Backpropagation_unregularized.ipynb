{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit #to calculate logit \n",
    "import scipy.io as si\n",
    "import numpy as np\n",
    "\n",
    "#.mat file has two matrix X (5000*400) and y(5000*1)\n",
    "File = si.loadmat('ex4data1.mat')\n",
    "\n",
    "#sigmoid function implementation\n",
    "def sigmoid_function(z):\n",
    "          return expit(z)\n",
    "\n",
    "#sigmoid gradient implementation\n",
    "def sigmoid_gradient(Z):\n",
    "    temp = sigmoid_function(Z)\n",
    "    return (temp*(1 - temp))\n",
    "\n",
    "#implement Forward Propagation\n",
    "def compute_input_layer(theta1,input_layer):\n",
    "    return (sigmoid_function(np.dot(input_layer,theta1.T)))\n",
    "\n",
    "def compute_hidden_layer(theta2,A):\n",
    "    return (sigmoid_function(np.dot(A,theta2.T)))\n",
    "\n",
    "#return 5000*10 matrix\n",
    "def compute_hypothesis(Theta1,Theta2,input_layer):\n",
    "    \n",
    "    #get 5000*25 matrix\n",
    "    A1 = compute_input_layer(Theta1,input_layer)\n",
    "    \n",
    "    #change A dim to 5000*26\n",
    "    A1 = np.insert(A1,0,1,axis = 1)\n",
    "\n",
    "    #get 5000*10 matrix\n",
    "    A2 = compute_hidden_layer(Theta2,A1)\n",
    "    return A2 , A1\n",
    "\n",
    "#implement Cost function for Neural Nertwork with no Regularization\n",
    "\n",
    "#calculate cost for a given training data and parameters\n",
    "def Cost_function_n_reg(theta1,theta2,input_layer,actual_result,K):\n",
    "    \n",
    "    #calculate hypothesis\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(theta1,theta2,input_layer)\n",
    "    \n",
    "    \n",
    "    term1 =  - np.sum(np.multiply(actual_result,np.log(hypothesis1))) \n",
    "    term2 =  - np.sum(np.multiply((1 -actual_result),np.log(1 - hypothesis1))) \n",
    "    cost = term1 + term2\n",
    "            \n",
    "    cost = (cost/(len(hypothesis1)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_gradient(np.array([-1, -0.5 ,0, 0.5, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input_layer\n",
    "input_layer = np.array(File['X'])\n",
    "input_layer = np.insert(input_layer,0,1,axis = 1)\n",
    "\n",
    "#randomly initializes theata1(25,401) and theta2(10,26) for a given epsilon\n",
    "\n",
    "\n",
    "epsilon = 0.12\n",
    "init_theta1 = np.random.uniform(-epsilon,epsilon,(25,401))\n",
    "init_theta2 = np.random.uniform(-epsilon,epsilon,(10,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n"
     ]
    }
   ],
   "source": [
    "print(init_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "#create actual result of training set in form of {0,1}\n",
    "def result_binary(actual_result,num_labels):\n",
    "    y = np.zeros((len(actual_result),num_labels))\n",
    "    for i in range(len(actual_result)):\n",
    "        temp = actual_result[i]\n",
    "        y[i][temp - 1] = 1\n",
    "    return y\n",
    "\n",
    "output_layer = result_binary(File['y'],10)\n",
    "print(output_layer[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "delta_3 = hypothesis1 - output_layer\n",
    "print(delta_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 25)\n"
     ]
    }
   ],
   "source": [
    "term1 = np.dot(delta_3,init_theta2).T\n",
    "term1 = np.delete(term1,0,0)\n",
    "delta_2 = np.multiply(term1.T,sigmoid_gradient(np.dot(input_layer,init_theta1.T)))\n",
    "\n",
    "print(delta_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "grad_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 26)\n",
      "(5000, 10)\n",
      "(10, 25)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis2.shape)\n",
    "print(delta_3.shape)\n",
    "grad_2 = np.dot(hypothesis2.T,delta_3)\n",
    "grad_2 = np.delete(grad_2,0,0).T\n",
    "print(grad_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement BackPropagation\n",
    "def BackPropagation(init_theta1,init_theta2,input_layer,output_layer,m):\n",
    "    \n",
    "    #calculate forward propagation\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "    \n",
    "    #computes small delta\n",
    "    \n",
    "    #compute delta_3\n",
    "    delta_3 = hypothesis1 - output_layer\n",
    "    \n",
    "    #compute delta_2\n",
    "    term1 = delta_3.dot(init_theta2)\n",
    "    delta_2 = term1*(hypothesis2*(1 - hypothesis2))\n",
    "    delta_2 = np.delete(delta_2,0,1)\n",
    "    \n",
    "    #compute gradient for layer 1\n",
    "    grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "    grad_1 = grad_1/m\n",
    "    \n",
    "    #compute gradient for layer 2\n",
    "    grad_2 = np.dot(hypothesis2.T,delta_3).T\n",
    "    grad_2 = grad_2/m\n",
    "    \n",
    "    return (grad_1 , grad_2)\n",
    "\n",
    "GRADIENT_1 , GRADIENT_2 = BackPropagation(init_theta1,init_theta2,input_layer,output_layer,len(File['X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "print(GRADIENT_1.shape,GRADIENT_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.0841471   0.09092974  0.014112  ]\n",
      " [ 1.         -0.07568025 -0.09589243 -0.02794155]\n",
      " [ 1.          0.06569866  0.09893582  0.04121185]\n",
      " [ 1.         -0.05440211 -0.09999902 -0.05365729]\n",
      " [ 1.          0.0420167   0.09906074  0.06502878]]\n",
      "[2 3 1 2 3]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "(5, 4)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "#perform  Gradient Checking on a small neural network\n",
    "\n",
    "#make a small neural network\n",
    "\n",
    "#define sizes\n",
    "input_layer_size = 3 #including bias unit\n",
    "hidden_layer_size = 5 #including bias unit\n",
    "num_labels = 3\n",
    "m = 5\n",
    "def debugInitializeWeights(fan_out,fan_in):\n",
    "    W = np.zeros((fan_out,fan_in + 1))\n",
    "    W = (np.sin(np.arange(1,np.size(W) + 1)).reshape(W.shape))/10\n",
    "    return W\n",
    "#put some random values\n",
    "#randomly  initializes test_theta's with a range of (-.12,.12)\n",
    "epsilon = 0.12\n",
    "test_theta1 = debugInitializeWeights(hidden_layer_size ,input_layer_size )\n",
    "test_theta2 = debugInitializeWeights(num_labels,hidden_layer_size )\n",
    "\n",
    "#randomly generate input layer (5*2)\n",
    "test_input_layer = debugInitializeWeights(m,input_layer_size - 1)\n",
    "test_input_layer = np.insert(test_input_layer,0,1,axis = 1)\n",
    "print(test_input_layer)\n",
    "\n",
    "#randomly generate  actual result \n",
    "test_result = 1 + np.remainder(np.arange(1,m+1),num_labels)\n",
    "print(test_result)\n",
    "\n",
    "#create test actualresult in binary form\n",
    "test_actual_result = np.zeros((m,num_labels))\n",
    "for i in range(m):\n",
    "         test_actual_result[i,test_result[i] - 1] = 1\n",
    "\n",
    "print(test_actual_result)\n",
    "\n",
    "#make a long test_THETA VECTOR \n",
    "\n",
    "print(test_theta1.shape)\n",
    "print(test_theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make calls to backpropagation and Cost_function for small neural network\n",
    "TEST_GRADIENT_1 , TEST_GRADIENT_2 = BackPropagation(test_theta1,test_theta2,test_input_layer,test_actual_result,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(test_theta1,test_theta2,epsilon,num_labels,test_input_layer,test_result):\n",
    "    \n",
    "    GRADIENT_CHECK = np.zeros((len(test_theta1.flatten()) + len(test_theta2.flatten()),1))\n",
    "    test_theta_L = np.concatenate((test_theta1.flatten(),test_theta2.flatten()),0)\n",
    "    test_theta_L = test_theta_L.reshape(len(test_theta1.flatten()) + len(test_theta2.flatten()),1)\n",
    "    \n",
    "    for i in range(len(test_theta_L)):\n",
    "        inc = np.zeros((len(test_theta_L),1))\n",
    "        inc[i] = inc[i] + epsilon\n",
    "    \n",
    "        theta_plus = test_theta_L + inc\n",
    "        theta_minus = test_theta_L - inc\n",
    "    \n",
    "        #unrolling for theta_plus\n",
    "        pass_theta1 = np.array(theta_plus[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "        pass_theta2 = np.array(theta_plus[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "    \n",
    "        cost_plus = Cost_function_n_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels)\n",
    "    \n",
    "        #unrolling for theta_minus\n",
    "        pass_theta1 = np.array(theta_minus[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "        pass_theta2 = np.array(theta_minus[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "    \n",
    "        cost_minus = Cost_function_n_reg(pass_theta1,pass_theta2,test_input_layer,test_result,num_labels)\n",
    "    \n",
    "        GRADIENT_CHECK[i] = (cost_plus - cost_minus)/(2*epsilon)\n",
    "        \n",
    "    return GRADIENT_CHECK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from Backpropagtion for theta 1 \n",
      "\n",
      "[[  1.23162247e-02   1.73828184e-04   2.61455144e-04   1.08701450e-04]\n",
      " [  3.92471369e-03   1.90101252e-04   2.22272331e-04   5.00872547e-05]\n",
      " [ -8.08459407e-03   3.13170587e-05  -2.17840341e-05  -5.48569864e-05]\n",
      " [ -1.26669105e-02  -1.56130210e-04  -2.45506163e-04  -1.09164881e-04]\n",
      " [ -5.59342547e-03  -2.00036572e-04  -2.43630220e-04  -6.32313673e-05]]\n",
      "\n",
      "output from Gradient Checking for theta 1 \n",
      "\n",
      "[[  1.23162247e-02   1.73828185e-04   2.61455142e-04   1.08701450e-04]\n",
      " [  3.92471368e-03   1.90101255e-04   2.22272329e-04   5.00872543e-05]\n",
      " [ -8.08459406e-03   3.13170601e-05  -2.17840368e-05  -5.48569856e-05]\n",
      " [ -1.26669105e-02  -1.56130211e-04  -2.45506164e-04  -1.09164882e-04]\n",
      " [ -5.59342546e-03  -2.00036570e-04  -2.43630223e-04  -6.32313690e-05]]\n",
      "\n",
      "output from Backpropagtion for theta 2 \n",
      "\n",
      "[[ 0.30934772  0.16106714  0.14703652  0.15826858  0.15761671  0.14723636]\n",
      " [ 0.108133    0.05616337  0.05195105  0.05473534  0.05530828  0.05177526]\n",
      " [ 0.10627037  0.0557611   0.05055681  0.05388051  0.05474072  0.05029295]]\n",
      "\n",
      "output from Gradient Checking for theta 2 \n",
      "\n",
      "[[ 0.30934772  0.16106714  0.14703652  0.15826858  0.15761671  0.14723636]\n",
      " [ 0.108133    0.05616337  0.05195105  0.05473534  0.05530828  0.05177526]\n",
      " [ 0.10627037  0.0557611   0.05055681  0.05388051  0.05474072  0.05029295]]\n"
     ]
    }
   ],
   "source": [
    "#make calls to backpropagation and Cost_function for small neural network\n",
    "TEST_GRADIENT_1 , TEST_GRADIENT_2 = BackPropagation(test_theta1,test_theta2,test_input_layer,test_actual_result,m)\n",
    "\n",
    "GRADIENT_CHECK = gradient_checking(test_theta1,test_theta2,.0001,num_labels,test_input_layer,test_actual_result)\n",
    "#COMPARE OUTPUT OF GRADIENT_CHECKING AND BACKPROPAGATION\n",
    "\n",
    "GRADIENT_CHECK1 = np.array(GRADIENT_CHECK[0:test_theta1.shape[0]*test_theta1.shape[1]]).reshape(test_theta1.shape)\n",
    "GRADIENT_CHECK2 = np.array(GRADIENT_CHECK[test_theta1.shape[0]*test_theta1.shape[1]:]).reshape(test_theta2.shape)\n",
    "\n",
    "print(\"output from Backpropagtion for theta 1 \\n\")\n",
    "print(TEST_GRADIENT_1)\n",
    "print(\"\\noutput from Gradient Checking for theta 1 \\n\")\n",
    "print(GRADIENT_CHECK1)\n",
    "\n",
    "print(\"\\noutput from Backpropagtion for theta 2 \\n\")\n",
    "print(TEST_GRADIENT_2)\n",
    "print(\"\\noutput from Gradient Checking for theta 2 \\n\")\n",
    "print(GRADIENT_CHECK2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn parameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
