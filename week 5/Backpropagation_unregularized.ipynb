{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit #to calculate logit \n",
    "import scipy.io as si\n",
    "import numpy as np\n",
    "\n",
    "#.mat file has two matrix X (5000*400) and y(5000*1)\n",
    "File = si.loadmat('ex4data1.mat')\n",
    "\n",
    "#sigmoid function implementation\n",
    "def sigmoid_function(z):\n",
    "          return expit(z)\n",
    "\n",
    "#sigmoid gradient implementation\n",
    "def sigmoid_gradient(Z):\n",
    "    temp = sigmoid_function(Z)\n",
    "    return (temp*(1 - temp))\n",
    "\n",
    "#implement Forward Propagation\n",
    "def compute_input_layer(theta1,input_layer):\n",
    "    return (sigmoid_function(np.dot(input_layer,theta1.T)))\n",
    "\n",
    "def compute_hidden_layer(theta2,A):\n",
    "    return (sigmoid_function(np.dot(A,theta2.T)))\n",
    "\n",
    "#return 5000*10 matrix\n",
    "def compute_hypothesis(Theta1,Theta2,input_layer):\n",
    "    \n",
    "    #get 5000*25 matrix\n",
    "    A1 = compute_input_layer(Theta1,input_layer)\n",
    "    \n",
    "    #change A dim to 5000*26\n",
    "    A1 = np.insert(A1,0,1,axis = 1)\n",
    "\n",
    "    #get 5000*10 matrix\n",
    "    A2 = compute_hidden_layer(Theta2,A1)\n",
    "    return A2 , A1\n",
    "\n",
    "#implement Cost function for Neural Nertwork with no Regularization\n",
    "\n",
    "#return the actual result of Kth classifier for a given data in training set\n",
    "def get_result(actual_result,K,data):\n",
    "    if actual_result[data] == K:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#return the value for Kth classifier for a given data in training set\n",
    "def get_hypo_value_K(hypothesis,data,K):\n",
    "        return (hypothesis[data].item(K))\n",
    "\n",
    "#calculate cost for a given training data and parameters\n",
    "def Cost_function_n_reg(theta1,theta2,input_layer,actual_result,K):\n",
    "    \n",
    "    #calculate hypothesis\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(theta1,theta2,input_layer)\n",
    "    \n",
    "    cost = 0\n",
    "    #iterate it for each training data\n",
    "    for i in range(len(hypothesis1)):\n",
    "        #for each class\n",
    "        for j in range(K):\n",
    "            term1 =  - get_result(actual_result,j,i)*np.log(get_hypo_value_K(hypothesis1,i,j)) \n",
    "            term2 =  - (1 - get_result(actual_result,j,i))*np.log(1 - get_hypo_value_K(hypothesis1,i,j)) \n",
    "            cost = cost + term1 + term2\n",
    "            \n",
    "    cost = (cost/(len(hypothesis1)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_gradient(np.array([-1, -0.5 ,0, 0.5, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input_layer\n",
    "input_layer = np.array(File['X'])\n",
    "input_layer = np.insert(input_layer,0,1,axis = 1)\n",
    "\n",
    "#randomly initializes theata1(25,401) and theta2(10,26) for a given epsilon\n",
    "\n",
    "\n",
    "epsilon = 0.12\n",
    "init_theta1 = np.random.uniform(-epsilon,epsilon,(25,401))\n",
    "init_theta2 = np.random.uniform(-epsilon,epsilon,(10,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401)\n"
     ]
    }
   ],
   "source": [
    "print(init_theta1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "#create actual result of training set in form of {0,1}\n",
    "def result_binary(actual_result,num_labels):\n",
    "    y = np.zeros((len(actual_result),num_labels))\n",
    "    for i in range(len(actual_result)):\n",
    "        temp = actual_result[i]\n",
    "        y[i][temp - 1] = 1\n",
    "    return y\n",
    "\n",
    "output_layer = result_binary(File['y'],10)\n",
    "print(output_layer[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "delta_3 = hypothesis1 - output_layer\n",
    "print(delta_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 25)\n"
     ]
    }
   ],
   "source": [
    "term1 = np.dot(delta_3,init_theta2).T\n",
    "term1 = np.delete(term1,0,0)\n",
    "delta_2 = np.multiply(term1.T,sigmoid_gradient(np.dot(input_layer,init_theta1.T)))\n",
    "\n",
    "print(delta_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "grad_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 26)\n",
      "(5000, 10)\n",
      "(10, 25)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis2.shape)\n",
    "print(delta_3.shape)\n",
    "grad_2 = np.dot(hypothesis2.T,delta_3)\n",
    "grad_2 = np.delete(grad_2,0,0).T\n",
    "print(grad_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement BackPropagation\n",
    "def BackPropagation(init_theta1,init_theta2,input_layer,output_layer):\n",
    "    \n",
    "    #calculate forward propagation\n",
    "    hypothesis1 , hypothesis2 = compute_hypothesis(init_theta1,init_theta2,input_layer)\n",
    "    \n",
    "    #computes small delta\n",
    "    \n",
    "    #compute delta_3\n",
    "    delta_3 = hypothesis1 - output_layer\n",
    "    \n",
    "    #compute delta_2\n",
    "    term1 = np.dot(delta_3,init_theta2).T\n",
    "    term1 = np.delete(term1,0,0)\n",
    "    delta_2 = np.multiply(term1.T,sigmoid_gradient(np.dot(input_layer,init_theta1.T)))\n",
    "    \n",
    "    #compute gradient for layer 1\n",
    "    grad_1 = np.dot(input_layer.T,delta_2).T\n",
    "    grad_1 = grad_1/len(hypothesis1)\n",
    "    \n",
    "    #compute gradient for layer 2\n",
    "    grad_2 = np.dot(hypothesis2.T,delta_3)\n",
    "    grad_2 = np.delete(grad_2,0,0).T\n",
    "    grad_2 = grad_2/len(hypothesis2)\n",
    "    \n",
    "    return (grad_1 , grad_2)\n",
    "\n",
    "GRADIENT_1 , GRADIENT_2 = BackPropagation(init_theta1,init_theta2,input_layer,output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 25)\n"
     ]
    }
   ],
   "source": [
    "print(GRADIENT_1.shape,GRADIENT_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.37472727  0.94687881]\n",
      " [ 1.          0.98595169  0.9216116 ]\n",
      " [ 1.          0.77952787  0.40017177]\n",
      " [ 1.          0.44324102  0.17411526]\n",
      " [ 1.          0.78086834  0.88055597]]\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "(4, 3)\n",
      "(3, 5)\n",
      "[-0.11088972 -0.03380308 -0.07400609 -0.09408612 -0.04424503  0.09828427\n",
      "  0.06051384 -0.11566431  0.03934134 -0.01766719 -0.03754778 -0.00017925\n",
      "  0.05500626  0.00336233 -0.07651638  0.01897128  0.02481286 -0.05106702\n",
      " -0.04654575  0.10761832  0.10874479  0.0407346   0.0454665  -0.03393324\n",
      "  0.00912121  0.06582694  0.11768603]\n"
     ]
    }
   ],
   "source": [
    "#perform  Gradient Checking on a small neural network\n",
    "\n",
    "#make a small neural network\n",
    "\n",
    "#define sizes\n",
    "Lambda = 0\n",
    "input_layer_size = 3 #including bias unit\n",
    "hidden_layer_size = 5 #including bias unit\n",
    "num_labels = 3\n",
    "m = 5\n",
    "\n",
    "#put some random values\n",
    "#randomly  initializes test_theta's with a range of (-.12,.12)\n",
    "epsilon = 0.12\n",
    "test_theta1 = np.random.uniform(-epsilon,epsilon,(hidden_layer_size -1,input_layer_size ))\n",
    "test_theta2 = np.random.uniform(-epsilon,epsilon,(num_labels,hidden_layer_size ))\n",
    "\n",
    "#randomly generate input layer (5*2)\n",
    "test_input_layer = np.random.rand(m,input_layer_size - 1)\n",
    "test_input_layer = np.insert(test_input_layer,0,1,axis = 1)\n",
    "print(test_input_layer)\n",
    "\n",
    "#randomly generate  actual result \n",
    "test_result = np.random.randint(0,3,(m,1))\n",
    "print(test_result)\n",
    "\n",
    "#create test actualresult in binary form\n",
    "test_actual_result = np.zeros((len(test_result),num_labels))\n",
    "for i in range(len(test_result)):\n",
    "         test_actual_result[i][test_result[i]] = 1\n",
    "\n",
    "print(test_actual_result)\n",
    "\n",
    "#make a long test_THETA VECTOR \n",
    "test_theta_L = np.concatenate((test_theta1.flatten(),test_theta2.flatten()))\n",
    "print(test_theta1.shape)\n",
    "print(test_theta2.shape)\n",
    "print(test_theta_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make calls to backpropagation and Cost_function for small neural network\n",
    "TEST_GRADIENT_1 , TEST_GRADIENT_2 = BackPropagation(test_theta1,test_theta2,test_input_layer,test_actual_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14796640752\n"
     ]
    }
   ],
   "source": [
    "print(Cost_function_n_reg(test_theta1,test_theta2,test_input_layer,test_result,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(test_theta_L,epsilon):\n",
    "    GRADIENT_CHECK = np.zeros((len(test_theta_L),1))\n",
    "    for i in range(len(test_theta_L)):\n",
    "        inc = np.zeros((len(test_theta_L),1))\n",
    "        inc[i] = inc[i] + epsilon\n",
    "    \n",
    "        theta_plus = test_theta_L + inc\n",
    "        theta_minus = test_theta_L - inc\n",
    "    \n",
    "        #unrolling for theta_plus\n",
    "        pass_theta1 = np.array(theta_plus[0:12]).reshape(4,3)\n",
    "        pass_theta2 = np.array(theta_plus[12:]).reshape(3,5)\n",
    "    \n",
    "        cost_plus = Cost_function_n_reg(pass_theta1,pass_theta2,test_input_layer,test_result,3)\n",
    "    \n",
    "        #unrolling for theta_minus\n",
    "        pass_theta1 = np.array(theta_minus[0:12]).reshape(4,3)\n",
    "        pass_theta2 = np.array(theta_minus[12:]).reshape(3,5)\n",
    "    \n",
    "        cost_minus = Cost_function_n_reg(pass_theta1,pass_theta2,test_input_layer,test_result,3)\n",
    "    \n",
    "        GRADIENT_CHECK[i] = (cost_plus - cost_minus)/(2*epsilon)\n",
    "    \n",
    "    return GRADIENT_CHECK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from Backpropagtion for theta 1 \n",
      "\n",
      "[[-0.00833061 -0.00633859 -0.00483304]\n",
      " [ 0.01516642  0.01294939  0.0074928 ]\n",
      " [ 0.01589434  0.01203138  0.00930053]\n",
      " [ 0.01701806  0.01168611  0.01108773]]\n",
      "\n",
      "output from Gradient Checking for theta 1 \n",
      "\n",
      "[[-0.00833061 -0.00633859 -0.00483304]\n",
      " [ 0.01516642  0.01294939  0.0074928 ]\n",
      " [ 0.01589434  0.01203138  0.00930053]\n",
      " [ 0.01701806  0.01168611  0.01108773]]\n",
      "\n",
      "output from Backpropagtion for theta 2 \n",
      "\n",
      "[[-0.13220065 -0.13858658 -0.14322924 -0.14119313]\n",
      " [ 0.14306104  0.15018876  0.15523379  0.15288642]\n",
      " [ 0.24140897  0.25784969  0.26679253  0.25990196]]\n",
      "\n",
      "output from Gradient Checking for theta 2 \n",
      "\n",
      "[[-0.13220065 -0.13858658 -0.14322924 -0.14119313]\n",
      " [ 0.14306104  0.15018876  0.15523379  0.15288642]\n",
      " [ 0.24140897  0.25784969  0.26679253  0.25990196]]\n"
     ]
    }
   ],
   "source": [
    "test_theta_L = test_theta_L.reshape(len(test_theta1.flatten()) + len(test_theta2.flatten()) ,1)\n",
    "GRADIENT_CHECK = gradient_checking(test_theta_L,.0001)\n",
    "#COMPARE OUTPUT OF GRADIENT_CHECKING AND BACKPROPAGATION\n",
    "\n",
    "GRADIENT_CHECK1 = np.array(GRADIENT_CHECK[0:12]).reshape(4,3)\n",
    "GRADIENT_CHECK2 = np.array(GRADIENT_CHECK[12:]).reshape(3,5)\n",
    "GRADIENT_CHECK2 = np.delete(GRADIENT_CHECK2,0,1)\n",
    "\n",
    "print(\"output from Backpropagtion for theta 1 \\n\")\n",
    "print(TEST_GRADIENT_1)\n",
    "print(\"\\noutput from Gradient Checking for theta 1 \\n\")\n",
    "print(GRADIENT_CHECK1)\n",
    "\n",
    "print(\"\\noutput from Backpropagtion for theta 2 \\n\")\n",
    "print(TEST_GRADIENT_2)\n",
    "print(\"\\noutput from Gradient Checking for theta 2 \\n\")\n",
    "print(GRADIENT_CHECK2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn parameters "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
